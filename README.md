# WSL Assistant ğŸš€  
*Run a fully integrated AI and database stack on your low-end, CPU-only Windows machineâ€”secure, resilient, and privacy-focused.*  

WSL Assistant brings **on-device AI** to **low-power systems** by leveraging WSL2 and Debian. Unlike traditional AI setups that require **power-hungry GPUs**, this project is built **for CPUs only**â€”allowing you to run **efficient local inference** and a **full RAG pipeline** on even modest hardware.  

With WSL Assistant, you can **self-host AI assistants, chatbots, and knowledge graphs** without relying on external cloud APIs. The lightweight stack includes:  
âœ… **llama.cpp** for blazing-fast **CPU inference**  
âœ… **Redis Stack** for vector database storage  
âœ… **Neo4j** for knowledge graphs  
âœ… **Optimized for low-end Windows laptops & edge devices**  

With a memory footprint as low as **1GB**, WSL Assistant lets you run **local AI inference and an entire RAG pipeline on a 10-year-old device!**  

![WSL2](https://img.shields.io/badge/WSL2-Supported-blue)  ![Debian](https://img.shields.io/badge/Debian-Supported-blue)  ![License](https://img.shields.io/badge/License-MIT-green)  

![Memory](https://raw.githubusercontent.com/rajatasusual/wsl-assistant/refs/heads/master/assets/mem.png)  

---

## **1. Table of Contents**
- [1. Table of Contents](#1-table-of-contents)
- [2. Overview](#2-overview)
    - [2.1 Why This Project Exists?](#21-why-this-project-exists)
    - [2.2 Why WSL and Debian?](#22-why-wsl-and-debian)
    - [2.3 What Are the Components and Their Purpose?](#23-what-are-the-components-and-their-purpose)
- [3. Use Cases](#3-use-cases)
- [4. Features](#4-features)
    - [4.1 Security](#41-security)
    - [4.2 Resiliency](#42-resiliency)
    - [4.3 Privacy](#43-privacy)
    - [4.4 CPU-Optimized AI-Powered RAG Pipeline](#44-cpu-optimized-ai-powered-rag-pipeline)
- [5. Prerequisites](#5-prerequisites)
- [6. Installation](#6-installation)
- [7. Start the Services](#7-start-the-services)
- [8. Performance Benchmarks](#8-performance-benchmarks)
- [9. Roadmap](#9-roadmap)
- [10. Common Issues & Fixes](#10-common-issues--fixes)
- [11. Contributing](#11-contributing)
- [12. License](#12-license)
- [13. Credits](#13-credits)

---

## **2. Overview**  

### **2.1 Why This Project Exists?**  
Most AI frameworks are built for **high-end GPUs**, making them **unusable on older machines** or **low-power edge devices**. WSL Assistant solves this by providing:  
- **A fully integrated CPU-based AI environment** for local inference.  
- **A lightweight stack (only ~1GB RAM)** to run AI models without lag.  
- **An optimized RAG pipeline** with Redis + Neo4j for AI-powered retrieval.  
- **No cloud dependency**â€”your data stays **local and private**.  

### **2.2 Why WSL and Debian?**  
- **WSL2**: Runs Linux on Windows with near-native performance.  
- **Debian**: Stable, secure, and lightweightâ€”ideal for low-resource machines.  
- **No need for Docker**: Avoids container overhead while still maintaining isolation.  

### **2.3 What Are the Components and Their Purpose?**  
| Component      | Purpose |
|---------------|---------|
| **Redis Stack**  | Fast vector database for AI embeddings |
| **Neo4j**       | Graph database for knowledge management |
| **llama.cpp**   | Local AI inference engine (CPU-optimized) |
| **Fail2Ban & UFW** | Security hardening against brute-force attacks |
| **Systemd Auto-Restart** | Ensures services are resilient to crashes |

---

## **3. Use Cases**  
- **AI-powered local search**: Store and retrieve knowledge with Redis + Neo4j.  
- **Self-hosted AI assistant**: Use `llama.cpp` for **CPU-only inference**.  
- **Privacy-first chatbots**: Avoid OpenAI/Google APIsâ€”run everything **offline**.  
- **Edge AI applications**: Ideal for **low-end devices** with limited compute.  

---

## **4. Features**  

### **4.1 Security** ğŸ”’  
âœ… **Disables root SSH login**  
âœ… **Enables firewall (ufw) and Fail2Ban** to prevent attacks  
âœ… **Automatic security updates**  

### **4.2 Resiliency** ğŸ”„  
âœ… **Auto-restarts essential services** after crashes  
âœ… **Persistent Redis storage** for AI embeddings  
âœ… **WSL2 optimizations for memory efficiency**  

### **4.3 Privacy** ğŸ›¡ï¸  
âœ… No external cloud dependencies  
âœ… Local **AI inference with llama.cpp**  
âœ… **Fully air-gapped if needed**  

### **4.4 CPU-Optimized AI-Powered RAG Pipeline** ğŸ¤–  
1ï¸âƒ£ **User query â†’** Sent to `llama.cpp` (CPU-only)  
2ï¸âƒ£ **Query transformed â†’** Passed to Redis vector DB  
3ï¸âƒ£ **Relevant knowledge retrieved â†’** From Neo4j knowledge graph  
4ï¸âƒ£ **Final AI response â†’** AI generates response  

---

## **5. Prerequisites**  
- **Windows 10/11 with WSL2** enabled  
- **At least 4GB RAM** (8GB recommended for AI inference with >1b parameter models)  
- **At least 20GB free disk space**  

---

## **6. Installation**  

```bash
# Install WSL2 and Debian
wsl --install -d Debian

# Clone this repo
git clone https://github.com/rajatasusual/wsl-assistant.git
cd wsl-assistant

# Run setup script
./setup.sh
```

For **detailed setup instructions**, see [INSTALLATION.md](INSTALLATION.md).  
For **frequently seen issues**, see [FAQs.md](FAQs.md).

---

## **7. Start the Services**  

```bash
# Start all required services
./start.sh
```

For troubleshooting, see logs in `/var/log/wsl-assistant/`.

---

## **8. Performance Benchmarks**  

WSL Assistant is optimized for **low-latency CPU-only inference**. Below is a benchmark of `llama 3B Q8_0` on a low-end WSL2 setup.  

### **Test System Specs:**  
ğŸ“Œ **RAM:** 4GB  
ğŸ“Œ **Storage:** 20GB  
ğŸ“Œ **Processor:** AMD Z1 Extreme  
ğŸ“Œ **# Cores:** 4  
ğŸ“Œ **GPU:** âŒ (Noneâ€”CPU-only inference)  
ğŸ“Œ **OS:** Debian (WSL2)  

### **Tokens Per Second (T/s) Benchmarks:**  

| Model            | Size      | Params    | Backend | Threads | Test Type | Tokens/sec (Â± std) |
|-----------------|----------:|----------:|---------|--------:|----------:|-------------------:|
| **LLaMA 3B Q8_0** | 366.8MB  | 361.82M  | CPU     | 2       | pp512     | 253.07 Â± 23.75    |
| **LLaMA 3B Q8_0** | 366.8MB  | 361.82M  | CPU     | 2       | tg128     | 54.44 Â± 4.87      |

![Tokens per second](assets/tpt.png)

### **Interpreting These Results**  
âœ… **Blazing fast inference on just a CPU!**  
âœ… **Low power consumptionâ€”runs smoothly on a 4GB RAM system!**  
âœ… **Ideal for low-end edge devices & offline AI assistants!**  

---

## **9. Roadmap** ğŸš€  
- [ ] Web UI for monitoring services  
- [ ] Dockerized version for portability  
- [ ] Integration with **LangChain** for extended RAG  
- [ ] Multi-user SSH with isolated environments  

---

## **10. Common Issues & Fixes**  

â“ **Q: Redis Stack Server is not starting**  
âœ… **Fix**: Run `sudo systemctl restart redis-stack-server`  

â“ **Q: "Out of memory" error when loading models**  
âœ… **Fix**:  
- Try a smaller GGUF model (`SmolLM2-360M-Instruct.q8.gguf`)  
- Increase **WSL2 memory**: Edit `.wslconfig` and set:  
  ```ini
  [wsl2]
  memory=8GB
  ```

---

## **11. Contributing**  
Contributions are welcome!  

1. Fork the repository  
2. Create a new branch (`feature-xyz`)  
3. Submit a **pull request**  

---

## **12. License**  
This project is licensed under the **MIT License**.  

---

## **13. Credits**  
Shout-out to:  
- **[llama.cpp](https://github.com/ggml-org/llama.cpp)** for CPU-based AI inference  
- **[Redis](https://redis.io/)** for vector storage  
- **[Neo4j](https://neo4j.com/)** for knowledge graphs  
- **The WSL Community** for making Linux on Windows seamless  
